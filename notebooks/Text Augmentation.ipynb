{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from checklist.editor import Editor\n",
    "editor = Editor()\n",
    "from transformers import MarianMTModel, MarianTokenizer, pipeline\n",
    "\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "processor = spacy.load('en_core_web_sm')\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action\n",
    "\n",
    "\n",
    "target_model_name = 'Helsinki-NLP/opus-mt-en-roa'\n",
    "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
    "target_pipe = pipeline(\"text2text-generation\", model=target_model, tokenizer=target_tokenizer, framework=\"pt\", device=0)\n",
    "\n",
    "\n",
    "en_model_name = 'Helsinki-NLP/opus-mt-roa-en'\n",
    "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
    "en_model = MarianMTModel.from_pretrained(en_model_name)\n",
    "en_pipe = pipeline(\"text2text-generation\", model=en_model, tokenizer=en_tokenizer, framework=\"pt\", device=0)\n",
    "\n",
    "\n",
    "\n",
    "def synonym_w2v(premise, hypothesis):\n",
    "    \"\"\"relpace random words with synonym\"\"\"\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', lang='eng', aug_p=.2)\n",
    "    if random.random() < .5:\n",
    "        premise =  aug.augment(premise)\n",
    "    else:\n",
    "        hypothesis = aug.augment(premise)\n",
    "    return premise, hypothesis\n",
    "\n",
    "\n",
    "def bert_replacement(premise, hypothesis):\n",
    "    \"\"\"pick random word. replace in both premise and hypothesis using bert\"\"\"\n",
    "    try:\n",
    "        if random.random() < .5:\n",
    "            doc = processor(premise)\n",
    "        else:\n",
    "            doc = processor(hypothesis)\n",
    "\n",
    "        word_idx = np.random.randint(len(doc))\n",
    "        while (str(doc[word_idx]).lower() in ['a', 'the', 'an', ',', '.', 'in', 'to', 'on', 'is']):\n",
    "            word_idx = np.random.randint(len(doc))\n",
    "        masked_text = re.sub(r'\\b%s\\b' % str(doc[word_idx]), '{mask}', doc.text, flags=re.I)\n",
    "        suggestion = random.choice(editor.suggest(masked_text, nsamples=10))\n",
    "        if type(suggestion) == tuple:\n",
    "            suggestion = suggestion[0]\n",
    "        aug_premise = re.sub(r'\\b%s\\b' % str(doc[word_idx]), suggestion, premise, flags=re.I)\n",
    "        aug_hypothesis = re.sub(r'\\b%s\\b' % str(doc[word_idx]), suggestion, hypothesis, flags=re.I)\n",
    "    except:\n",
    "        aug_premise =  premise\n",
    "        aug_hypothesis = hypothesis\n",
    "    return aug_premise, aug_hypothesis\n",
    "\n",
    "    \n",
    "def bert_insertion(premise, hypothesis):\n",
    "    \"\"\"insert words in sentence based on context\"\"\"\n",
    "    aug = naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-uncased', action=\"insert\", aug_p=.2)\n",
    "    if random.random() < .5:\n",
    "        premise =  aug.augment(premise)\n",
    "    else:\n",
    "        hypothesis = aug.augment(premise)\n",
    "    return premise, hypothesis\n",
    "\n",
    "\n",
    "def translate(texts, pipe, language=\"es\"):\n",
    "    template = lambda text: f\"{text}\" if language == \"en\" else f\">>{language}<< {text}\"\n",
    "    src_texts = [template(text) for text in texts]    \n",
    "    return [ex['generated_text'] for ex in pipe(src_texts)]\n",
    "\n",
    "\n",
    "def back_translation(premise, hypothesis, source_lang=\"en\", target_lang=\"es\"):\n",
    "    \"\"\"augment text by backtranslation\"\"\"\n",
    "    # Translate from source to target language\n",
    "    translated_texts = translate([premise, hypothesis], target_pipe, language=target_lang)\n",
    "\n",
    "    # Translate from target language back to source language\n",
    "    aug_premise, aug_hypothesis = translate(translated_texts, en_pipe, language=source_lang)\n",
    "    return aug_premise, aug_hypothesis\n",
    "\n",
    "\n",
    "def augment_example(row):\n",
    "    \"\"\"perform random augmentation\"\"\"\n",
    "    row = row.copy(deep=True)\n",
    "    premise = row.premise\n",
    "    hypothesis = row.hypothesis\n",
    "    label = row.label\n",
    "    \n",
    "    augmentations = ['synonym_w2v', 'bert_replacement', 'bert_insertion', 'back_translation']\n",
    "    \n",
    "    \n",
    "    aug_type = random.choice(augmentations)\n",
    "    if aug_type == 'synonym_w2v':\n",
    "        premise, hypothesis = synonym_w2v(premise, hypothesis)\n",
    "    elif aug_type == 'bert_replacement':\n",
    "        premise, hypothesis = bert_replacement(premise, hypothesis)\n",
    "    elif aug_type == 'bert_insertion':\n",
    "        premise, hypothesis = bert_insertion(premise, hypothesis)\n",
    "    elif aug_type == 'back_translation':\n",
    "        premise, hypothesis = back_translation(premise, hypothesis)\n",
    "    \n",
    "    row.premise = premise\n",
    "    row.hypothesis = hypothesis\n",
    "    row['aug_type'] = aug_type # for debugging\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/eric/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Reusing dataset snli (/home/eric/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Reusing dataset snli (/home/eric/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset('snli', split='train').to_pandas()\n",
    "dev = load_dataset('snli', split='validation').to_pandas()\n",
    "test = load_dataset('snli', split='test').to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 6min 25s, sys: 38min 53s, total: 7h 45min 18s\n",
      "Wall time: 1h 19min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_aug = train.sample(15000).apply(augment_example, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.666666666666664"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aug.to_csv('datasets/general_train_augmentations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug.to_csv('../datasets/general_train_augmentations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.37724000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "210158.064 / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_translation    3773\n",
       "bert_insertion      3769\n",
       "synonym_w2v         3738\n",
       "bert_replacement    3720\n",
       "Name: aug_type, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aug.aug_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/eric/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9123554bfc466c8d8278f5bd19cc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.concat([train.sample(20000), train_aug.drop(columns='aug_type')]).to_csv('../datasets/mixed_augmentations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bcc30624bff1e9d0\n",
      "Reusing dataset csv (/home/eric/.cache/huggingface/datasets/csv/default-bcc30624bff1e9d0/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5903fa609d418486d8aeca8d21049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'premise', 'hypothesis', 'label'],\n",
       "    num_rows: 35000\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(path='csv', data_files='../datasets/mixed_augmentations.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bcc30624bff1e9d0\n",
      "Reusing dataset csv (/home/eric/.cache/huggingface/datasets/csv/default-bcc30624bff1e9d0/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44a213ede7b48e882eec341d7fb70c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'] = load_dataset(path='csv', data_files='../datasets/mixed_augmentations.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{tabular}{||c c c c||} \n",
       " \\hline\n",
       " Col1 & Col2 & Col2 & Col3 \\\\ [0.5ex] \n",
       " \\hline\\hline\n",
       " 1 & 6 & 87837 & 787 \\\\ \n",
       " \\hline\n",
       " 2 & 7 & 78 & 5415 \\\\\n",
       " \\hline\n",
       " 3 & 545 & 778 & 7507 \\\\\n",
       " \\hline\n",
       " 4 & 545 & 18744 & 7560 \\\\\n",
       " \\hline\n",
       " 5 & 88 & 788 & 6344 \\\\ [1ex] \n",
       " \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{tabular}{||c c c c||} \n",
    " \\hline\n",
    " Col1 & Col2 & Col2 & Col3 \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " 1 & 6 & 87837 & 787 \\\\ \n",
    " \\hline\n",
    " 2 & 7 & 78 & 5415 \\\\\n",
    " \\hline\n",
    " 3 & 545 & 778 & 7507 \\\\\n",
    " \\hline\n",
    " 4 & 545 & 18744 & 7560 \\\\\n",
    " \\hline\n",
    " 5 & 88 & 788 & 6344 \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\documentclass{article}\n",
       "\\usepackage{array}\n",
       "\\begin{document}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{ | m{5em} | m{1cm}| m{1cm} | } \n",
       "  \\hline\n",
       "  cell1 dummy text dummy text dummy text& cell2 & cell3 \\\\ \n",
       "  \\hline\n",
       "  cell1 dummy text dummy text dummy text & cell5 & cell6 \\\\ \n",
       "  \\hline\n",
       "  cell7 & cell8 & cell9 \\\\ \n",
       "  \\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{document}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\documentclass{article}\n",
    "\\usepackage{array}\n",
    "\\begin{document}\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ | m{5em} | m{1cm}| m{1cm} | } \n",
    "  \\hline\n",
    "  cell1 dummy text dummy text dummy text& cell2 & cell3 \\\\ \n",
    "  \\hline\n",
    "  cell1 dummy text dummy text dummy text & cell5 & cell6 \\\\ \n",
    "  \\hline\n",
    "  cell7 & cell8 & cell9 \\\\ \n",
    "  \\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\\end{document}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
